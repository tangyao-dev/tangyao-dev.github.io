<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hbase安装部署</title>
    <url>/2019/11/03/hbase%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>hbase的安装部署主要包括zookeeper的安装配置和hbase的安装配置两步</p>
<p>详细步骤请点击阅读全文</p>
<a id="more"></a>

<p>以下操作均需要在所有机子上完成</p>
<ol>
<li><p>zookeeper安装配置</p>
<ol>
<li><p>下载并解压zookeeper</p>
<p>解压命令：<code>tar -C /hadoop/ -vxf apache-zookeeper-3.5.6-bin.tar</code></p>
<p>为了方便操作，将文件夹改名为zookeeper-3.5.6:</p>
<p><code>mv apache-zookeeper-3.5.6-bin  zookeeper-3.5.6</code></p>
</li>
<li><p>配置zookeeper环境变量</p>
<p>打开etc/profile文件：<br><code>vi /etc/profile</code></p>
<p>在文件末尾添加zookeeper变量信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#set zookeeper ervironment</span><br><span class="line">ZOOKEEPER_HOME=/hadoop/zookeeper-3.5.6</span><br><span class="line">PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br></pre></td></tr></table></figure>

<p>使环境变量立即生效：<br><code>source /etc/profile</code></p>
</li>
<li><p>修改zookeeper配置文件</p>
<p>进入zookeeper的config目录，复制zoo_sample.cfg文件，并改名为zoo.cfg：</p>
<p><code>cp zoo_sample.cfg  zoo.cfg</code></p>
<p>根据需要修改zoo.cfg文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tickTime=2000                     </span><br><span class="line">dataDir=/hadoop/zookeeper/data </span><br><span class="line">clientPort=2181                   </span><br><span class="line">initLimit=5                       </span><br><span class="line">syncLimit=2  		       </span><br><span class="line">server.A=B:C:D</span><br></pre></td></tr></table></figure>

<p>配置信息说明：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tickTime：Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔</span><br><span class="line">dataDir：写数据的日志文件保存目录</span><br><span class="line">clientPort：客户端连接 Zookeeper服务器的端口，Zookeeper会监听这个端口接受客户端的访问请求</span><br><span class="line">initLimit：Zookeeper接受客户端初始化连接时最长能忍受多少个心跳时间间隔数</span><br><span class="line">syncLimit：标识 Leader 与 Follower 之间发送消息，请求和应答时间长度</span><br><span class="line">server.A=B:C:D  ：A是一个数字，表示这个是第几号服务器；B是这个服务器的ip地址；C表示的是这个服务器与集群中的Leader服务器交换信息的端口；D表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</span><br><span class="line">例如：server.1=master:2888:3888</span><br></pre></td></tr></table></figure>

<p>在dataDir设定的目录下创建myid文件，各机器分别将zoo.cfg文件中server.A=B:C:D这一项中的A值写道myid文件中，只写这个数字即可。</p>
</li>
<li><p>启动zookeeper</p>
<p>在所有机器上执行：</p>
<p><code>zkServer.sh start</code></p>
<p>启动后，可通过<code>zkServer.sh status</code>命令查看leader是哪台机器</p>
</li>
</ol>
</li>
<li><p>hbase安装配置</p>
<ol>
<li><p>下载并解压hbase</p>
<p>解压命令：<code>tar -C /hadoop/ -vxf hbase-2.2.2-bin.tar.gz</code></p>
<p>为了方便操作，将文件夹改名为hbase-2.2.2:</p>
<p><code>mv hbase-2.2.2-bin  hbase-2.2.2</code></p>
</li>
<li><p>配置hbase环境变量</p>
<p>打开etc/profile文件：<br><code>vi /etc/profile</code></p>
<p>在文件末尾添加zookeeper变量信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#set hbase ervironment</span><br><span class="line">export HBASE_HOME=/hadoop/hbase-2.2.2</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br></pre></td></tr></table></figure>

<p>使环境变量立即生效：<br><code>source /etc/profile</code></p>
</li>
<li><p>修改hbase配置文件</p>
<p>进入hbase的config目录，根据需要修改hbase-enc.sh文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/java/jdk1.8.0_151</span><br><span class="line">export HBASE_LOG_DIR=/var/log/hbase</span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure>

<p>配置信息说明：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JAVA_HOME：安装的jdk路径，均需配置</span><br><span class="line">HBASE_LOG_DIR：日志文件存储地址</span><br><span class="line">HBASE_MANAGES_ZK：如果使用HBase自带的Zookeeper值设成true，如果使用自己安装的Zookeeper需要将该值设为false</span><br></pre></td></tr></table></figure>

<p>将zookeeper的配置文件拷贝到hbase的conf目录：</p>
<p><code>cp /hadoop/zookeeper-3.5.6/conf/zoo.cfg    /hadoop/hbase-2.2.2/conf/</code></p>
<p>配置hbase-site.xml文件：在configuration中间添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.rootdir&lt;/name&gt;  </span><br><span class="line">        &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">       &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;master,datanode&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.master.info.bindAddress&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;0.0.0.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.master.info.port&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;16010&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.master.port&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;16000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>配置信息说明：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase.rootdir：Hbase数据存储目录</span><br><span class="line">hbase.cluster.distributed：是否是完全分布式模式，单机模式和伪分布式模式需要将该值设为false</span><br><span class="line">hbase.zookeeper.quorum: zookeeper的集群，多台机器以逗号分隔（建议使用单数）</span><br><span class="line">Hhbase.master.info.bindAddress  Base Master web: 界面绑定的地址 默认: 0.0.0.0</span><br><span class="line">hbase.master.info.port HBase Master web: 界面端口.设置为-1 意味着你不想让他运行,默认: 60010</span><br><span class="line">hbase.master.port: Hbase的Master的端口,默认: 60000</span><br></pre></td></tr></table></figure>

<p>打开hbase的conf目录，修改regionservers文件，加入hadoop集群的 namenode节点和datanode节点的主机名，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">master</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动hbase</p>
<p>之前我们已经启动了hadoop集群和zookeeper，因此直接启动hbase即可，注意启动hbase的操作只需要master主机执行：</p>
<p><code>start-hbase.sh</code></p>
<p>若启动成功，在浏览器输入master:16010可以看到所有节点均出现在该网页上。</p>
<p>关闭时先由master关闭hbase，再由每个机器分别关闭自己的zookeeper，最后关闭hadoop集群。</p>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>配置</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>hive安装部署</title>
    <url>/2019/11/03/hive%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>hive的安装前置条件为hadoop必须已经安装配置完成</p>
<p>hive的安装步骤主要包括mysql的安装配置和hive安装配置两步</p>
<p>详细步骤请点击查看全文</p>
<a id="more"></a>

<ol>
<li><p>安装mysql</p>
<p>这里采用离线安装的方式。</p>
<ol>
<li><p>下载linux版本的mysql软件并解压</p>
<p><a href="https://dev.mysql.com/downloads/mysql/" target="_blank" rel="noopener">mysql下载地址</a>，选择对应的linux版本进行下载。</p>
<p>下载后使用如下命令解压：</p>
<p><code>tar ‐xvf mysql‐5.7.25‐1.el6.x86_64.rpm‐bundle.tar</code></p>
</li>
<li><p>卸载旧版本的mysql</p>
<p>查看是否有旧版本的mysql存在，命令如下：</p>
<p><code>rpm -qa | grep mysql</code></p>
<p><code>rpm -qa | grep mariadb</code></p>
<p>如果存在进入目录删除即可，不存在不用进行任何操作</p>
</li>
<li><p>安装mysql</p>
<p>解压后的mysql安装包里包含四个rpm包，这四个包之间有依赖关系，需要严格按照顺序依次安装，顺序为common→libs→client→server，安装命令依次为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql‐community‐common‐5.7.25‐1.el6.x86_64.rpm </span><br><span class="line">mysql‐community‐libs‐5.7.25‐1.el6.x86_64.rpm </span><br><span class="line">mysql‐community‐client‐5.7.25‐1.el6.x86_64.rpm </span><br><span class="line">mysql‐community‐server‐5.7.25‐1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>配置mysql</p>
<ol>
<li><p>设置mysql密码</p>
<p>启动mysql服务：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable mysqld.service</span><br></pre></td></tr></table></figure>

<p>设置mysql密码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql ‐u root </span><br><span class="line">use mysql; </span><br><span class="line">update user set password=password(&apos;password&apos;) where user=&apos;root&apos;; </span><br><span class="line">quit;</span><br></pre></td></tr></table></figure>

<p>如果设置的密码强度较低，可能会报错，可进入/etc/my.cnf文件，加入如下key-value值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">validate_password_policy=LOW</span><br><span class="line">validate_password_length=6</span><br></pre></td></tr></table></figure>

<p>登录mysql：</p>
<p><code>mysql -u root –p</code></p>
</li>
<li><p>开通mysql远程访问</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;123456&apos; with grant option;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建hive数据库及用户</p>
<p>创建hive数据库：</p>
<p><code>CREATE DATABASE hive;</code></p>
<p>创建用户：</p>
<p><code>useradd hive passwd hive</code></p>
<p>授权(注意在mysql界面输入)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE USER &apos;hive&apos; IDENTIFIED BY &apos;hive&apos;;</span><br><span class="line">grant all privileges on *.* to &apos;hive&apos; identified by &apos;hive&apos;;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>
</li>
<li><p>上传mysql驱动jar包至/hadoop/hive-2.3.6/lib目录下</p>
</li>
</ol>
</li>
<li><p>安装hive</p>
<ol>
<li><p>下载并解压hive安装包</p>
<p>当前Hive可到apache官网下载，下载后解压：</p>
<p><code>tar -C /hadoop/ -zxvf apache-hive-2.3.6-bin.tar.gz</code></p>
<p>为方便以后使用，解压后进行重命名：</p>
<p><code>mv apache-hive-2.3.6-bin/ hive-2.3.6</code></p>
</li>
<li><p>配置环境变量</p>
<p>打开/etc/profile文件：</p>
<p><code>vi /etc/profile</code></p>
<p>在末尾加上以下内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HADOOP_CONF_HOME=$HADOOP_HOME/etc/hadoop/</span><br><span class="line">export HIVE_HOME=/hadoop/hive-2.3.6</span><br><span class="line">export HIVE_CONF_DIR=/hadoop/hive-2.3.6/conf</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin:$HADOOP_HOME/bin</span><br></pre></td></tr></table></figure>

<p>使环境变量立即生效：</p>
<p><code>source /etc/profile</code></p>
</li>
<li><p>配置配置文件</p>
<p>首先进入hive-2.3.6/conf目录，对配置文件进行重命名：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd $HIVE_HOME/conf</span><br><span class="line">cp hive-env.sh.template hive-env.sh</span><br><span class="line">cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties</span><br><span class="line">cp hive-log4j2.properties.template hive-log4j2.properties</span><br></pre></td></tr></table></figure>

<p>修改hive-env.sh文件：在末尾添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/hadoop/hadoop-2.7.7</span><br><span class="line">export HIVE_CONF_DIR=/hadoop/hive-2.3.6/conf</span><br></pre></td></tr></table></figure>

<p>修改hive-site.xml文件：添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;jdbc:mysql://mysqlIP地址:3306/hive?createDatabaseIfNotExit=true&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;Username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;112233&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/usr/local/hive/tmp&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/usr/local/hive/tmp/resources&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;location of default database for the warehouse&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/tmp/hive&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.hbase.snapshot.restoredir&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/tmp&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;The directory in which to restore the HBase table snapshot.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.scratch.dir.permission&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;700&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;The permission for the user specific scratch directories that get created.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建本地目录及hdfs目录</p>
<p>创建本地目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir /hadoop/hive</span><br><span class="line">mkdir /hadoop/hive/tmp</span><br><span class="line">mkdir /hadoop/hive/tmp/resources</span><br></pre></td></tr></table></figure>

<p>创建hdfs目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir -p /tmp/hive</span><br><span class="line">hadoop fs -mkdir -p /apps/hive/warehouse</span><br></pre></td></tr></table></figure>
</li>
<li><p>初始化元数据库并测试是否可用</p>
<p>初始化元数据库：</p>
<p><code>schematool -dbType mysql -initSchema</code></p>
<p>测试hive是否可用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive</span><br><span class="line">show datatables;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -u root -p;</span><br><span class="line">use hive</span><br><span class="line">show tables;</span><br></pre></td></tr></table></figure>

<p>出现如下界面则证明配置成功：</p>
<img src="/.com//hive.jpg" style="zoom:75%;"></li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>配置</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop集群安装配置</title>
    <url>/2019/11/03/hadoop%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>配置步骤主要包括安装并配置jdk，设置SSH免密登录，安装并配置hadoop三步</p>
<p>详细步骤请阅读全文</p>
<a id="more"></a>

<p><em>以下步骤均需要在所有机子中进行操作</em></p>
<ol>
<li><p><strong>安装并配置jdk</strong></p>
<ol>
<li><p>下载jdk，并解压到usr/local目录下，解压命令:</p>
<p><code>tar -vxf jdk-8u151-linux-x64.tar.gz</code></p>
</li>
<li><p>配置java环境变量</p>
<p>打开/etc/profile文件</p>
<p><code>vi /etc/profile</code></p>
<p>在末尾添加java环境变量:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#set java enviroment</span><br><span class="line">export JAVA_HOME=/usr/local/java/jdk1.8.0_151 </span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/tools.jar </span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>使配置环境立即生效:</p>
<p><code>source /etc/profile</code></p>
</li>
<li><p>检查是否安装成功:</p>
<p><code>java -version</code></p>
<p>如果输出刚刚安装的jdk版本信息则安装成功，如果输出openjdk信息，说明安装的hadoop自带openjdk，需要将openjdk卸载，卸载后再次使环境变量生效，再次检查是否安装成功。</p>
</li>
</ol>
</li>
<li><p>设置SSH免密登录</p>
<ol>
<li><p>关闭防火墙:</p>
<p><code>systemctl stop firewalld</code></p>
<p><code>systemctl disable firewalld</code></p>
</li>
<li><p>关闭selinux</p>
<p>打开selinux文件:</p>
<p><code>vi /etc/sysconfig/selinux</code></p>
<p>将SELINUX的值改为disabled</p>
</li>
<li><p>修改hostname及host文件</p>
<p>修改hostname，一般一个机子做master，两个机子分别做slave1和slave2:</p>
<p><code>hostnamectl set-hostname xxx</code></p>
<p>修改/etc/hosts文件，将主机名和ip地址对应，命令如下：</p>
<p><code>vi /etc/hosts</code></p>
<p>在hosts中添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xxx.xxx.xx.xx master</span><br><span class="line">xxx.xxx.xx.xx slave1</span><br><span class="line">xxx.xxx.xx.xx slave2</span><br></pre></td></tr></table></figure>
</li>
<li><p>生成ssh密钥</p>
<p>输入如下命令生成key，不输入密码一直回车:</p>
<p><code>ssh-keygen -t rsa</code></p>
<p>/root文件夹下会自动生成.ssh文件夹</p>
</li>
<li><p>合并所有机器密钥</p>
<p>进入.ssh文件夹，将密钥写入到authorized_keys文件中，如果没有自动生成authorized_keys文件，自己创建即可，写入命令如下：</p>
<p><code>cat id_rsa.pub&gt;&gt;authorized_keys</code></p>
<p>将其他机子的密钥也合并到authorized_keys文件中</p>
</li>
<li><p>测试是否设置成功</p>
<p>输入:<code>ssh xxx</code></p>
<p>xxx为步骤2中设置的主机名，如果输入此条命令访问其他主机不需要输入密码，则证明设置成功。如果设置失败，在/var/logs文件夹下查看报错日志，常见的错误有root文件夹或.ssh文件夹权限错误，使用chmod命令修改权限即可。</p>
</li>
</ol>
</li>
<li><p>安装并配置hadoop</p>
<ol>
<li><p>下载hadoop，并解压到/hadoop文件夹下，解压命令:</p>
<p><code>tar -vxf hadoop-2.7.7.tar.gz</code></p>
</li>
<li><p>创建数据存放的文件夹:</p>
<p><code>mkdir /hadoop/tmp</code></p>
<p><code>mkdir /hadoop/hdfs</code></p>
<p><code>mkdir /hadoop/hdfs/data</code></p>
<p><code>mkdir /hadoop/hdfs/name</code></p>
</li>
<li><p>修改hadoop配置文件</p>
<p>修改hadoop-env.sh文件：在文件末尾添加:</p>
<p>export JAVA_HOME=/usr/local/java/jdk1.8.0_151（替换为你的jdk目录）</p>
<p>修改yarn-env.sh文件：在文件末尾添加:</p>
<p>export JAVA_HOME=/usr/local/java/jdk1.8.0_151（替换为你的jdk目录）</p>
<p>修改core-site.xml文件：在文件末尾添加:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;io.file.buffer.size&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;131072&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/hadoop/tmp&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>修改hdfs-site.xml文件：在文件末尾添加:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/hadoop/hdfs/name&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/hadoop/hdfs/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;slave1:9001&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>修改mapred-site.xml文件：在文件末尾添加:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>修改yarn-site.xml文件：在文件末尾添加:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8032&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8030&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8031&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8033&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8088&lt;/value&gt;                                       </span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<hr>
</li>
<li><p>配置hadoop环境变量</p>
<p>打开/etc/profile文件:</p>
<p><code>vi /etc/profile</code></p>
<p>在末尾添加hadoop环境变量:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#set hadoop enviroment</span><br><span class="line">      export HADOOP_HOME=/hadoop/hadoop-2.7.7</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<p>使配置环境立即生效:</p>
<p><code>source /etc/profile</code></p>
</li>
</ol>
</li>
<li><p>启动hadoop</p>
<p>格式化namnode，命令如下：</p>
<p><code>hdfs namenode -format</code></p>
<p><code>start-all.sh</code></p>
<p>测试是否启动成功，在浏览器输入网址</p>
<p>master:50070（将master改为master节点机器的IP地址）</p>
<p>master:8080（将master改为master节点机器的IP地址）</p>
<p>所有机器均出现在master:50070网址中则证明启动成功，hdfs文件也可以在这里查看。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>配置</tag>
      </tags>
  </entry>
</search>
