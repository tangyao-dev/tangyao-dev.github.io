<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>NLP基本概念及发展历史介绍</title>
    <url>/2019/12/05/NLP%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<p>本文基于宗成庆老师的《统计自然语言处理》一书，整理了自然语言处理领域的基本概念、所要研究的基本问题、研究目前面临的瓶颈、基本研究方法及研究现状。</p>
<p>详细内容请点击阅读全文</p>
<a id="more"></a>

<table>
<thead>
<tr>
<th>问题的提出</th>
<th></th>
<th></th>
<th>中文网页检索的最高准确率不足40%</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
<td></td>
<td>跨语言通讯和信息获取技术具有重要的用途</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>发现或挖掘社交网络，确定不同的实体、事件和知识之间的关联</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>让计算机能够自动或半自动地理解自然语言文本，懂得人的意图和心声</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>让计算机实现海量语言文本的自动处理、挖掘和有效利用，满足不同用户的各种需求，实现个性化信息服务</td>
<td></td>
</tr>
<tr>
<td>基本概念</td>
<td>相关学科简介</td>
<td>语言学</td>
<td>研究语言的本质、结构和发展规律的科学</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>语音学</td>
<td>语音和文字是语言的两个基本属性</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>计算语言学</td>
<td>人工智能和语言学的分支学科</td>
<td>通过建立形式化的计算模型来分析、理解和生成自然语言的学科</td>
</tr>
<tr>
<td></td>
<td></td>
<td>自然语言处理</td>
<td>研究如何利用计算机技术对语言文本（句子、篇章或话语等）进行处理和加工的一门学科</td>
<td>包括对词法、句法、语义和语用等信息的识别、分类、提取、转换和生成等各种处理方法和实现技术</td>
</tr>
<tr>
<td></td>
<td></td>
<td>自然语言理解</td>
<td>是探索人类自身语言能力和语言思 维活动的本质，研究模仿人类语言认知过程的自然语 言处理方法和实现技术的一门学科</td>
<td>表现(act)如何？ 反应(react)如何？ 相互作用(interact )如何？</td>
</tr>
<tr>
<td></td>
<td></td>
<td>中文信息处理</td>
<td>针对中文的自然语言处理技术</td>
<td></td>
</tr>
<tr>
<td></td>
<td>三个不同的语系</td>
<td>屈折语(fusional language/ inflectional language)</td>
<td>用词的形态变化表示语法关系</td>
<td>英语、法语</td>
</tr>
<tr>
<td></td>
<td></td>
<td>黏着语(agglutinative language)</td>
<td>词内有专门表示语法意义的附加成分，词根或词干与附加成分的结合不紧密</td>
<td>日语、韩语、土耳其语</td>
</tr>
<tr>
<td></td>
<td></td>
<td>孤立语(analytic language)(分析语, isolating language)</td>
<td>形态变化少，语法关系靠词序和虚词表示</td>
<td>汉语</td>
</tr>
<tr>
<td></td>
<td>人类语言技术（HLT）</td>
<td>应用目标</td>
<td>机器翻译(Machine translation, MT)</td>
<td>实现一种语言到另一种语言的自动翻译。</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>信息检索(Information retrieval)</td>
<td>也称情报检索，就是利用计算机系统从大量文档中找到符合用户需要的相关信息</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>自动文摘 (Automatic summarization / Automatic  abstracting)</td>
<td>将原文档的主要内容或某方面的内容提取出来，形成原文档的摘要</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>问答系统 (Question-answering system)</td>
<td>通过计算机系统对人提出的问题的理解，利用自动推理等手段，在有关知识资源中自动求解答案并做出相应的回答。</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>信息过滤 (Information filtering)</td>
<td>通过计算机系统自动识别和过滤那些满足特定条件 的文档信息。</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>信息抽取 (Information extraction)</td>
<td>从指定文档中或者海量文本中抽取出用户感兴趣的信息。</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>文档分类 (Document categorization)</td>
<td>其目的就是利用计算机系统对大量的文档按照一定的分类标准（例如，根据主题或内容划分等）实现自动归类。</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>情感分类(Sentimental classification)</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>文字编辑和自动校对(Automatic proofreading)</td>
<td>对文字拼写、用词、甚至语法、文档格式等进行自动检查、校对和编排。</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>语言教学 (Language teaching)</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>文字识别 (Character recognition)</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>语音识别 (automatic speech recognition, ASR)</td>
<td>将输入语音信号自动转换成书面文字。</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>文语转换/ 语音合成 (text-to-speech synthesis)</td>
<td>将书面文本自动转换成对应的语音表征。</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>说话人识别/认同/验证 (speaker recognition/  identification/ verification)</td>
<td>对一言语样品做声学分析，依此推断(确定或 验证)说话人的身份。</td>
</tr>
<tr>
<td>基本问题</td>
<td>形态学 (Morphology) 问题</td>
<td></td>
<td>单词的识别/ 汉语的分词问题。</td>
<td>研究词(word) 由有意义的基本单位－词素 (morphemes)的构成问题。</td>
</tr>
<tr>
<td></td>
<td>句法 (Syntax) 问题</td>
<td></td>
<td>研究句子结构成分之间的相互关系和组成句子 序列的规则 。</td>
<td>建立快速有效的句子结构分析方法</td>
</tr>
<tr>
<td></td>
<td>语义 (Semantics) 问题</td>
<td></td>
<td>研究如何从一个语句中词的意义，以及这些词 在该语句中句法结构中的作用来推导出该语句的意 义。</td>
<td></td>
</tr>
<tr>
<td></td>
<td>语用学(Pragmatics) 问题</td>
<td></td>
<td>研究在不同上下文中语句的应用，以及上下文 对语句理解所产生的影响。</td>
<td>是研究语义学未能涵盖 的那些意义。</td>
</tr>
<tr>
<td></td>
<td>语音学(Phonetics) 问题</td>
<td></td>
<td>研究语音特性、语音描述、分类及转写方法等</td>
<td></td>
</tr>
<tr>
<td>主要困难</td>
<td>大量歧义(ambiguity)现象</td>
<td>词法歧义、词性歧义、结构歧义、语义歧义、语音歧义、多音字及韵律等歧义</td>
<td>普遍存在的不确定性：词法、句法、语义、语用和语音各个层面 未知语言现象的不可预测性：新的词汇、新的术语、新的语义和语法无处不在 始终面临的数据不充分性：有限的语言集合永远无法涵盖开放的语言现象 语言知识表达的复杂性：语义知识的模糊性和错综复杂的关联性难以用常规方法有效地描述，为语义计算带来了极大的困难 机器翻译中映射单元的不对等性：词法表达不相同、句法结构不一致、语义概念不对等</td>
<td></td>
</tr>
<tr>
<td></td>
<td>大量未知语言现象</td>
<td>新词、人名、地名、术语等</td>
<td>新含义</td>
<td>新用法和新句型等</td>
</tr>
<tr>
<td>基本研究方法</td>
<td>理性主义</td>
<td>基于规则的分析方法建立符号处理系统</td>
<td>知识库＋推理系统 = NLP系统</td>
<td>理论基础：Chomsky 的文法理论</td>
</tr>
<tr>
<td></td>
<td>经验主义</td>
<td>基于大规模真实语料(语言数据)建立计算方法</td>
<td>语料库＋统计模型 = NLP系统</td>
<td>理论基础：统计学、信息论、机器学习</td>
</tr>
<tr>
<td></td>
<td>数据驱动的翻译方法（如SMT和 NMT）</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>基于统计的方法</td>
<td></td>
<td>根据贝叶斯公式求解使P值（概率）最大的C（目标句子）</td>
<td></td>
</tr>
<tr>
<td>研究现状</td>
<td>部分问题得到了解决，可以为人们提供辅助性帮助，如：专业领域文档翻译，电子词典，搜索引擎，文字录入等；</td>
<td></td>
<td>基础问题研究仍任重而道远，如：语义表示和 计算、高质量的自动翻译等；</td>
<td></td>
</tr>
</tbody></table>
<p>参考文献：</p>
<p>[1]宗成庆.统计自然语言处理[M].清华大学出版社:北京,2013:266.</p>
]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>技术所存在问题</tag>
        <tag>技术发展史</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>hive常用命令及Java连接</title>
    <url>/2019/11/08/hive%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8Ajava%E8%BF%9E%E6%8E%A5/</url>
    <content><![CDATA[<p>本文主要介绍hive与关系数据库的区别、hive常用命令（包括托管表与外部表、分区与分桶、动态分区等）、以及使用Java的jdbc连接hive进行操作。</p>
<p>详细内容请点击阅读全文</p>
<a id="more"></a>

<ol>
<li><p>hive与关系数据库的区别</p>
<p>在介绍hive的使用前，先对hive与传统关系数据库的异同做一个比较，了解一下为什么需要hive。</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th>HQL</th>
<th>SQL</th>
</tr>
</thead>
<tbody><tr>
<td align="left">数据存储</td>
<td>HDFS，Hbase</td>
<td>LocalFS</td>
</tr>
<tr>
<td align="left">数据格式</td>
<td>用户自定义</td>
<td>系统决定</td>
</tr>
<tr>
<td align="left">数据更新</td>
<td>不支持（覆盖之前数据）</td>
<td>支持</td>
</tr>
<tr>
<td align="left">索引</td>
<td>有（0.8版本后添加）</td>
<td>有</td>
</tr>
<tr>
<td align="left">执行</td>
<td>MapReduce（select * from table）</td>
<td>Executor</td>
</tr>
<tr>
<td align="left">执行延迟</td>
<td>高</td>
<td>低</td>
</tr>
<tr>
<td align="left">可扩展性</td>
<td>高（UDF、UDAF、UDTF）</td>
<td>低</td>
</tr>
<tr>
<td align="left">数据规模</td>
<td>大（TB级别）</td>
<td>小</td>
</tr>
<tr>
<td align="left">数据检查</td>
<td>读时模式</td>
<td>写时模式</td>
</tr>
</tbody></table>
</li>
<li><p>hive常用命令</p>
<p>2.1 托管表和外部表</p>
<p>创建并删除托管表和外部表。</p>
<p>托管表数据存储在仓库目录下，删除时会删除元数据和数据，而外部表数据可以存储在任何hdfs目录下，删除时只会删除元数据。</p>
<p>创建语句命令，托管表：</p>
<p><code>create table TenminData_tg(attr1 STRING)</code></p>
<p>外部表：</p>
<p><code>create table TenminData_tg(attr1 STRING) LOCATION &#39;路径&#39;;</code></p>
<p>元数据可在navicat连接的hive里看到。</p>
<p>2.2 数据分区</p>
<p>创建带分区的表，并比较分区与否对查询的效率影响。</p>
<p>创建带分区托管表语句：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> TenminData(attr1 <span class="keyword">STRING</span>,attr2 <span class="keyword">STRING</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> ( attr1 <span class="keyword">string</span>,attr2 <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure>

<p>创建分区：</p>
<p><code>alter table tenmindata add partition(attr1=&#39;a1&#39;,attr2=&#39;a2&#39;);</code></p>
<p>加载数据到分区：</p>
<p><code>load data local inpath &#39;/路径/*.bz2&#39; into table tenmindata partition(attr1=&#39;a1&#39;,attr2=&#39;a2&#39;);</code></p>
<p>查看分区：</p>
<p><code>show partitions tenmindata;</code></p>
<p>2.3 数据分桶</p>
<p>分桶语句：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> TenminData()</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> ( fc <span class="keyword">string</span>)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (FAN_NO) SORTED <span class="keyword">BY</span> (DATA_DATE <span class="keyword">ASC</span>) <span class="keyword">INTO</span> <span class="number">5</span> BUCKETS</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure>

<p>分桶建议让hive自动分桶，一般通过hash算法进行分桶。分桶可以在分区的基础上进行，进一步提高查询效率。分桶后可以进行取样操作：</p>
<p><code>SELECT * FROM TENMINDATA3 TABLESAMPLE(BUCKET 1 OUT OF 2 ON FAN_NO);</code></p>
<p><code>SELECT * FROM TENMINDATA3 TABLESAMPLE(50 persent/row/M);</code></p>
<p>2.4 指定存储文件格式</p>
<p>指定hive存储的文件格式，包括AVRO、ORC、PARQUET、RCFILE、SEQUENCEFILE和TEXTFILE，使用方法在建表语句后加上：</p>
<p><code>CLUSTERED BY (FAN_NO) SORTED BY (DATA_DATE ASC) INTO 5 BUCKETS
STORED AS RCFILE</code></p>
<p>导入fc=1风场的数据，采用CPU计算，六种文件格式效率对比如下：</p>
<table>
<thead>
<tr>
<th>存储格式</th>
<th>分桶后最大文件大小（M）</th>
<th>运行时间（S）</th>
</tr>
</thead>
<tbody><tr>
<td>AVRO</td>
<td>14.5</td>
<td>22.957</td>
</tr>
<tr>
<td>ORC</td>
<td>1.62</td>
<td>21.854</td>
</tr>
<tr>
<td>PARQUET</td>
<td>2.98</td>
<td>23.136</td>
</tr>
<tr>
<td>RCFILE</td>
<td>12.78</td>
<td>20.691</td>
</tr>
<tr>
<td>SEQUENCEFILE</td>
<td>9.95</td>
<td>20.663</td>
</tr>
<tr>
<td>TEXTFILE</td>
<td>9.17</td>
<td>22.171</td>
</tr>
</tbody></table>
<p>2.5 其他命令</p>
<p>使用insert overwrite select注入数据。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tenmindata4 <span class="keyword">partition</span>(fc=<span class="number">1</span>,fj=<span class="string">'WT02287'</span>) <span class="keyword">SELECT</span> DATASOURCE,FAN_NO,DATA_DATE,FAN_STATUS,WIND_SPEED,ROTOR_RS,RS,WIND_DIRECTION,YAW_ANGLE,BOX_T,BOX_BEARING_T,ENVIRON_T,WT_T,ROTOR_GROUP_T,A_PHASE_C,B_PHASE_C,C_PHASE_C,A_PHASE_V,B_PHASE_V,C_PHASE_V,MACHINE_FREQUENCY,REACTIVE_POWER,<span class="keyword">POWER</span>,POWER_FACTOR,TOTAL_POWER,TOTAL_GEN_TIME,DOWN_TIME,STANDBY_TIME,REMARK <span class="keyword">FROM</span> TENMINDATA <span class="keyword">WHERE</span> FC=<span class="number">1</span> <span class="keyword">AND</span> FJ=<span class="string">'WT02287'</span>;</span><br></pre></td></tr></table></figure>

<p>使用From的方式插入数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">FROM TENMINDATA </span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tenmindata4 <span class="keyword">partition</span>(fc=<span class="number">1</span>,fj=<span class="string">'WT02288'</span>)</span><br><span class="line"><span class="keyword">SELECT</span> DATASOURCE,FAN_NO,DATA_DATE,FAN_STATUS,WIND_SPEED,ROTOR_RS,RS,WIND_DIRECTION,YAW_ANGLE,BOX_T,BOX_BEARING_T,ENVIRON_T,WT_T,ROTOR_GROUP_T,A_PHASE_C,B_PHASE_C,C_PHASE_C,A_PHASE_V,B_PHASE_V,C_PHASE_V,MACHINE_FREQUENCY,REACTIVE_POWER,<span class="keyword">POWER</span>,POWER_FACTOR,TOTAL_POWER,TOTAL_GEN_TIME,DOWN_TIME,STANDBY_TIME,REMARK</span><br><span class="line"><span class="keyword">WHERE</span> FC=<span class="number">1</span> <span class="keyword">AND</span> FJ=<span class="string">'WT02288'</span>;</span><br></pre></td></tr></table></figure>

<p>2.6 动态分区的创建</p>
<p>使用静态分区插入数据时，每次都需要指定插入的分区的值，非常繁琐，如果使用动态分区，hive可以自动匹配分区的值，从而提高效率。</p>
<p>开启动态分区：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.exec.dynamic.partition=true;  </span><br><span class="line">#开启动态分区，默认是false</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line">#开启允许所有分区都是动态的，否则必须要有静态分区才能使用</span><br></pre></td></tr></table></figure>

<p>SQL语句：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> TenminData4 (  </span><br><span class="line">    &gt; <span class="string">`DATASOURCE`</span> <span class="keyword">string</span> ,</span><br><span class="line">    &gt; <span class="string">`FAN_NO`</span> <span class="keyword">string</span> ,</span><br><span class="line">    &gt; <span class="string">`DATA_DATE`</span> <span class="keyword">string</span>,</span><br><span class="line">    &gt; <span class="string">`FAN_STATUS`</span> <span class="keyword">double</span>,</span><br><span class="line">    &gt; <span class="string">`WIND_SPEED`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'风速'</span>,</span><br><span class="line">    &gt; <span class="string">`ROTOR_RS`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'电机转速'</span>,</span><br><span class="line">    &gt; <span class="string">`RS`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'桨叶转速'</span>,</span><br><span class="line">    &gt; <span class="string">`WIND_DIRECTION`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'风向'</span>,</span><br><span class="line">    &gt; <span class="string">`YAW_ANGLE`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'偏航角度'</span>,</span><br><span class="line">    &gt; <span class="string">`BOX_T`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'齿轮箱油温'</span>,</span><br><span class="line">    &gt; <span class="string">`BOX_BEARING_T`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'齿轮箱轴承油温'</span>,</span><br><span class="line">    &gt; <span class="string">`ENVIRON_T`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'环境温度'</span>,</span><br><span class="line">    &gt; <span class="string">`WT_T`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'机舱温度'</span>,</span><br><span class="line">    &gt; <span class="string">`ROTOR_GROUP_T`</span> <span class="keyword">double</span>  <span class="keyword">comment</span> <span class="string">'发电机温度'</span>,</span><br><span class="line">    &gt; <span class="string">`A_PHASE_C`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'A相电流'</span>,</span><br><span class="line">    &gt; <span class="string">`B_PHASE_C`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'B相电流'</span>,</span><br><span class="line">    &gt; <span class="string">`C_PHASE_C`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'C相电流'</span>,</span><br><span class="line">    &gt; <span class="string">`A_PHASE_V`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'A相电压'</span>,</span><br><span class="line">    &gt; <span class="string">`B_PHASE_V`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'B相电压'</span>,</span><br><span class="line">    &gt; <span class="string">`C_PHASE_V`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'C相电压'</span>,</span><br><span class="line">    &gt; <span class="string">`MACHINE_FREQUENCY`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'电机频率'</span>,</span><br><span class="line">    &gt; <span class="string">`REACTIVE_POWER`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'无功功率'</span>,</span><br><span class="line">    &gt; <span class="string">`POWER`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'有功功率'</span>,</span><br><span class="line">    &gt; <span class="string">`POWER_FACTOR`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'功率因素'</span>,</span><br><span class="line">    &gt; <span class="string">`TOTAL_POWER`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'总发电量'</span>,</span><br><span class="line">    &gt; <span class="string">`TOTAL_GEN_TIME`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'总发电时间'</span>,</span><br><span class="line">    &gt; <span class="string">`DOWN_TIME`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'故障时间'</span>,</span><br><span class="line">    &gt; <span class="string">`STANDBY_TIME`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'备用时间'</span>,</span><br><span class="line">    &gt; <span class="string">`REMARK`</span> <span class="keyword">double</span> <span class="keyword">comment</span> <span class="string">'备注'</span>)</span><br><span class="line">    &gt; PARTITIONED <span class="keyword">BY</span> ( fc <span class="keyword">string</span>,fj <span class="keyword">string</span>)</span><br><span class="line">    &gt; <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> Tenmindata4 <span class="keyword">partition</span> (fc, fj) <span class="keyword">select</span> * <span class="keyword">from</span> Tenmindata <span class="keyword">where</span> fc=<span class="string">'1'</span>;</span><br></pre></td></tr></table></figure>

<p>运行结果截图：</p>
<p> <img src="/2019/11/08/hive%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8Ajava%E8%BF%9E%E6%8E%A5/hive1.jpg" alt></p>
<p>从截图可以看到，我们不需要挨个指定fj的值，动态分区的方式可以帮助我们自动根据fj分区导入数据。从而大大提高效率。</p>
</li>
<li><p>使用java api在hive中创建表</p>
</li>
</ol>
<p>首先使用<code>hiveserver2</code>命令开启机子上的hive服务</p>
<p>代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HiveJDBCTest</span> </span>&#123;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String URL = <span class="string">"jdbc:hive2://IP地址:10000/default"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DRIVER = <span class="string">"org.apache.hive.jdbc.HiveDriver"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String PASSWORD = <span class="string">"hive"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String USER = <span class="string">"hive"</span>; </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    	<span class="keyword">try</span> &#123;</span><br><span class="line">            Class.forName(DRIVER);</span><br><span class="line">            Connection conn = DriverManager.getConnection(URL,USER,PASSWORD);</span><br><span class="line">            Statement st = conn.createStatement();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// create table</span></span><br><span class="line">            String sql = <span class="string">"create table TenminData3 (  `DATASOURCE` string ,"</span></span><br><span class="line">            		+ <span class="string">"`FAN_NO` string ,"</span></span><br><span class="line">            		+ <span class="string">"`DATA_DATE` string,"</span></span><br><span class="line">            		+ <span class="string">"`FAN_STATUS` double,"</span></span><br><span class="line">            		+ <span class="string">"`WIND_SPEED` double,"</span></span><br><span class="line">            		+ <span class="string">"`ROTOR_RS` double,"</span></span><br><span class="line">            		+ <span class="string">"`RS` double,"</span></span><br><span class="line">            		+ <span class="string">"`WIND_DIRECTION` double,"</span></span><br><span class="line">            		+ <span class="string">"`YAW_ANGLE` double,"</span></span><br><span class="line">            		+ <span class="string">"`BOX_T` double,"</span></span><br><span class="line">            		+ <span class="string">"`BOX_BEARING_T` double,"</span></span><br><span class="line">            		+ <span class="string">"`ENVIRON_T` double,"</span></span><br><span class="line">            		+ <span class="string">"`WT_T` double,"</span></span><br><span class="line">            		+ <span class="string">"`ROTOR_GROUP_T` double,"</span></span><br><span class="line">            		+ <span class="string">"`A_PHASE_C` double,"</span></span><br><span class="line">            		+ <span class="string">"`B_PHASE_C` double,"</span></span><br><span class="line">            		+ <span class="string">"`C_PHASE_C` double,"</span></span><br><span class="line">            		+ <span class="string">"`A_PHASE_V` double,"</span></span><br><span class="line">            		+ <span class="string">"`B_PHASE_V` double,"</span></span><br><span class="line">            		+ <span class="string">"`C_PHASE_V` double,"</span></span><br><span class="line">            		+ <span class="string">"`MACHINE_FREQUENCY` double,"</span></span><br><span class="line">            		+ <span class="string">"`REACTIVE_POWER` double,"</span></span><br><span class="line">            		+ <span class="string">"`POWER` double,"</span></span><br><span class="line">            		+ <span class="string">"`POWER_FACTOR` double,"</span></span><br><span class="line">            		+ <span class="string">"`TOTAL_POWER` double,"</span></span><br><span class="line">            		+ <span class="string">"`TOTAL_GEN_TIME` double,"</span></span><br><span class="line">            		+ <span class="string">"`DOWN_TIME` double,"</span></span><br><span class="line">            		+ <span class="string">"`STANDBY_TIME` double,"</span></span><br><span class="line">            		+ <span class="string">"`REMARK` double)"</span></span><br><span class="line">            		+ <span class="string">"PARTITIONED BY ( fc string,fj string)"</span></span><br><span class="line">            		+ <span class="string">"row format delimited fields terminated by ','"</span>;</span><br><span class="line">            st.execute(sql);</span><br><span class="line">            <span class="comment">// show tables</span></span><br><span class="line">            sql = <span class="string">"show tables"</span>;</span><br><span class="line">            ResultSet rs = st.executeQuery(sql);</span><br><span class="line">            <span class="keyword">while</span>(rs.next())&#123;</span><br><span class="line">                System.out.println(rs.getString(<span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// describe table</span></span><br><span class="line">            sql = <span class="string">"desc tenmindata3"</span>;</span><br><span class="line">            rs = st.executeQuery(sql);</span><br><span class="line">            <span class="keyword">while</span>(rs.next())&#123;</span><br><span class="line">                System.out.println(rs.getString(<span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行结果截图：</p>
<p><img src="/2019/11/08/hive%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8Ajava%E8%BF%9E%E6%8E%A5/hive2.jpg" alt></p>
<p><img src="/2019/11/08/hive%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8Ajava%E8%BF%9E%E6%8E%A5/hive3.jpg" alt></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hive</tag>
        <tag>命令</tag>
        <tag>java api</tag>
      </tags>
  </entry>
  <entry>
    <title>hbase安装部署</title>
    <url>/2019/11/03/hbase%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>hbase的安装部署主要包括zookeeper的安装配置和hbase的安装配置两步</p>
<p>详细步骤请点击阅读全文</p>
<a id="more"></a>

<p>以下操作均需要在所有机子上完成</p>
<ol>
<li><p>zookeeper安装配置</p>
<ol>
<li><p>下载并解压zookeeper</p>
<p>解压命令：<code>tar -C /hadoop/ -vxf apache-zookeeper-3.5.6-bin.tar</code></p>
<p>为了方便操作，将文件夹改名为zookeeper-3.5.6:</p>
<p><code>mv apache-zookeeper-3.5.6-bin  zookeeper-3.5.6</code></p>
</li>
<li><p>配置zookeeper环境变量</p>
<p>打开etc/profile文件：<br><code>vi /etc/profile</code></p>
<p>在文件末尾添加zookeeper变量信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#set zookeeper ervironment</span><br><span class="line">ZOOKEEPER_HOME=/hadoop/zookeeper-3.5.6</span><br><span class="line">PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br></pre></td></tr></table></figure>

<p>使环境变量立即生效：<br><code>source /etc/profile</code></p>
</li>
<li><p>修改zookeeper配置文件</p>
<p>进入zookeeper的config目录，复制zoo_sample.cfg文件，并改名为zoo.cfg：</p>
<p><code>cp zoo_sample.cfg  zoo.cfg</code></p>
<p>根据需要修改zoo.cfg文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tickTime=2000                     </span><br><span class="line">dataDir=/hadoop/zookeeper/data </span><br><span class="line">clientPort=2181                   </span><br><span class="line">initLimit=5                       </span><br><span class="line">syncLimit=2  		       </span><br><span class="line">server.A=B:C:D</span><br></pre></td></tr></table></figure>

<p>配置信息说明：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tickTime：Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔</span><br><span class="line">dataDir：写数据的日志文件保存目录</span><br><span class="line">clientPort：客户端连接 Zookeeper服务器的端口，Zookeeper会监听这个端口接受客户端的访问请求</span><br><span class="line">initLimit：Zookeeper接受客户端初始化连接时最长能忍受多少个心跳时间间隔数</span><br><span class="line">syncLimit：标识 Leader 与 Follower 之间发送消息，请求和应答时间长度</span><br><span class="line">server.A=B:C:D  ：A是一个数字，表示这个是第几号服务器；B是这个服务器的ip地址；C表示的是这个服务器与集群中的Leader服务器交换信息的端口；D表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</span><br><span class="line">例如：server.1=master:2888:3888</span><br></pre></td></tr></table></figure>

<p>在dataDir设定的目录下创建myid文件，各机器分别将zoo.cfg文件中server.A=B:C:D这一项中的A值写道myid文件中，只写这个数字即可。</p>
</li>
<li><p>启动zookeeper</p>
<p>在所有机器上执行：</p>
<p><code>zkServer.sh start</code></p>
<p>启动后，可通过<code>zkServer.sh status</code>命令查看leader是哪台机器</p>
</li>
</ol>
</li>
<li><p>hbase安装配置</p>
<ol>
<li><p>下载并解压hbase</p>
<p>解压命令：<code>tar -C /hadoop/ -vxf hbase-2.2.2-bin.tar.gz</code></p>
<p>为了方便操作，将文件夹改名为hbase-2.2.2:</p>
<p><code>mv hbase-2.2.2-bin  hbase-2.2.2</code></p>
</li>
<li><p>配置hbase环境变量</p>
<p>打开etc/profile文件：<br><code>vi /etc/profile</code></p>
<p>在文件末尾添加zookeeper变量信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#set hbase ervironment</span><br><span class="line">export HBASE_HOME=/hadoop/hbase-2.2.2</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br></pre></td></tr></table></figure>

<p>使环境变量立即生效：<br><code>source /etc/profile</code></p>
</li>
<li><p>修改hbase配置文件</p>
<p>进入hbase的config目录，根据需要修改hbase-enc.sh文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/java/jdk1.8.0_151</span><br><span class="line">export HBASE_LOG_DIR=/var/log/hbase</span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure>

<p>配置信息说明：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JAVA_HOME：安装的jdk路径，均需配置</span><br><span class="line">HBASE_LOG_DIR：日志文件存储地址</span><br><span class="line">HBASE_MANAGES_ZK：如果使用HBase自带的Zookeeper值设成true，如果使用自己安装的Zookeeper需要将该值设为false</span><br></pre></td></tr></table></figure>

<p>将zookeeper的配置文件拷贝到hbase的conf目录：</p>
<p><code>cp /hadoop/zookeeper-3.5.6/conf/zoo.cfg    /hadoop/hbase-2.2.2/conf/</code></p>
<p>配置hbase-site.xml文件：在configuration中间添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.rootdir&lt;/name&gt;  </span><br><span class="line">        &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">       &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;master,datanode&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.master.info.bindAddress&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;0.0.0.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.master.info.port&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;16010&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.master.port&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;16000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>配置信息说明：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase.rootdir：Hbase数据存储目录</span><br><span class="line">hbase.cluster.distributed：是否是完全分布式模式，单机模式和伪分布式模式需要将该值设为false</span><br><span class="line">hbase.zookeeper.quorum: zookeeper的集群，多台机器以逗号分隔（建议使用单数）</span><br><span class="line">Hhbase.master.info.bindAddress  Base Master web: 界面绑定的地址 默认: 0.0.0.0</span><br><span class="line">hbase.master.info.port HBase Master web: 界面端口.设置为-1 意味着你不想让他运行,默认: 60010</span><br><span class="line">hbase.master.port: Hbase的Master的端口,默认: 60000</span><br></pre></td></tr></table></figure>

<p>打开hbase的conf目录，修改regionservers文件，加入hadoop集群的 namenode节点和datanode节点的主机名，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">master</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动hbase</p>
<p>之前我们已经启动了hadoop集群和zookeeper，因此直接启动hbase即可，注意启动hbase的操作只需要master主机执行：</p>
<p><code>start-hbase.sh</code></p>
<p>若启动成功，在浏览器输入master:16010可以看到所有节点均出现在该网页上。</p>
<p>关闭时先由master关闭hbase，再由每个机器分别关闭自己的zookeeper，最后关闭hadoop集群。</p>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hbase</tag>
        <tag>配置</tag>
      </tags>
  </entry>
  <entry>
    <title>hive安装部署</title>
    <url>/2019/11/03/hive%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<p>hive的安装前置条件为hadoop必须已经安装配置完成</p>
<p>hive的安装步骤主要包括mysql的安装配置和hive安装配置两步</p>
<p>详细步骤请点击查看全文</p>
<a id="more"></a>

<ol>
<li><p>安装mysql</p>
<p>这里采用离线安装的方式。</p>
<ol>
<li><p>下载linux版本的mysql软件并解压</p>
<p><a href="https://dev.mysql.com/downloads/mysql/" target="_blank" rel="noopener">mysql下载地址</a>，选择对应的linux版本进行下载。</p>
<p>下载后使用如下命令解压：</p>
<p><code>tar ‐xvf mysql‐5.7.25‐1.el6.x86_64.rpm‐bundle.tar</code></p>
</li>
<li><p>卸载旧版本的mysql</p>
<p>查看是否有旧版本的mysql存在，命令如下：</p>
<p><code>rpm -qa | grep mysql</code></p>
<p><code>rpm -qa | grep mariadb</code></p>
<p>如果存在进入目录删除即可，不存在不用进行任何操作</p>
</li>
<li><p>安装mysql</p>
<p>解压后的mysql安装包里包含四个rpm包，这四个包之间有依赖关系，需要严格按照顺序依次安装，顺序为common→libs→client→server，安装命令依次为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql‐community‐common‐5.7.25‐1.el6.x86_64.rpm </span><br><span class="line">mysql‐community‐libs‐5.7.25‐1.el6.x86_64.rpm </span><br><span class="line">mysql‐community‐client‐5.7.25‐1.el6.x86_64.rpm </span><br><span class="line">mysql‐community‐server‐5.7.25‐1.el6.x86_64.rpm</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>配置mysql</p>
<ol>
<li><p>设置mysql密码</p>
<p>启动mysql服务：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable mysqld.service</span><br></pre></td></tr></table></figure>

<p>设置mysql密码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql ‐u root </span><br><span class="line">use mysql; </span><br><span class="line">update user set password=password(&apos;password&apos;) where user=&apos;root&apos;; </span><br><span class="line">quit;</span><br></pre></td></tr></table></figure>

<p>如果设置的密码强度较低，可能会报错，可进入/etc/my.cnf文件，加入如下key-value值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">validate_password_policy=LOW</span><br><span class="line">validate_password_length=6</span><br></pre></td></tr></table></figure>

<p>登录mysql：</p>
<p><code>mysql -u root –p</code></p>
</li>
<li><p>开通mysql远程访问</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;123456&apos; with grant option;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建hive数据库及用户</p>
<p>创建hive数据库：</p>
<p><code>CREATE DATABASE hive;</code></p>
<p>创建用户：</p>
<p><code>useradd hive passwd hive</code></p>
<p>授权(注意在mysql界面输入)：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE USER &apos;hive&apos; IDENTIFIED BY &apos;hive&apos;;</span><br><span class="line">grant all privileges on *.* to &apos;hive&apos; identified by &apos;hive&apos;;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>
</li>
<li><p>上传mysql驱动jar包至/hadoop/hive-2.3.6/lib目录下</p>
</li>
</ol>
</li>
<li><p>安装hive</p>
<ol>
<li><p>下载并解压hive安装包</p>
<p>当前Hive可到apache官网下载，下载后解压：</p>
<p><code>tar -C /hadoop/ -zxvf apache-hive-2.3.6-bin.tar.gz</code></p>
<p>为方便以后使用，解压后进行重命名：</p>
<p><code>mv apache-hive-2.3.6-bin/ hive-2.3.6</code></p>
</li>
<li><p>配置环境变量</p>
<p>打开/etc/profile文件：</p>
<p><code>vi /etc/profile</code></p>
<p>在末尾加上以下内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HADOOP_CONF_HOME=$HADOOP_HOME/etc/hadoop/</span><br><span class="line">export HIVE_HOME=/hadoop/hive-2.3.6</span><br><span class="line">export HIVE_CONF_DIR=/hadoop/hive-2.3.6/conf</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin:$HADOOP_HOME/bin</span><br></pre></td></tr></table></figure>

<p>使环境变量立即生效：</p>
<p><code>source /etc/profile</code></p>
</li>
<li><p>配置配置文件</p>
<p>首先进入hive-2.3.6/conf目录，对配置文件进行重命名：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd $HIVE_HOME/conf</span><br><span class="line">cp hive-env.sh.template hive-env.sh</span><br><span class="line">cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties</span><br><span class="line">cp hive-log4j2.properties.template hive-log4j2.properties</span><br></pre></td></tr></table></figure>

<p>修改hive-env.sh文件：在末尾添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HADOOP_HOME=/hadoop/hadoop-2.7.7</span><br><span class="line">export HIVE_CONF_DIR=/hadoop/hive-2.3.6/conf</span><br></pre></td></tr></table></figure>

<p>修改hive-site.xml文件：添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;jdbc:mysql://mysqlIP地址:3306/hive?createDatabaseIfNotExit=true&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;Username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;112233&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/usr/local/hive/tmp&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/usr/local/hive/tmp/resources&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;location of default database for the warehouse&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/tmp/hive&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.hbase.snapshot.restoredir&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/tmp&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;The directory in which to restore the HBase table snapshot.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hive.scratch.dir.permission&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;700&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;The permission for the user specific scratch directories that get created.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建本地目录及hdfs目录</p>
<p>创建本地目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir /hadoop/hive</span><br><span class="line">mkdir /hadoop/hive/tmp</span><br><span class="line">mkdir /hadoop/hive/tmp/resources</span><br></pre></td></tr></table></figure>

<p>创建hdfs目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir -p /tmp/hive</span><br><span class="line">hadoop fs -mkdir -p /apps/hive/warehouse</span><br></pre></td></tr></table></figure>
</li>
<li><p>初始化元数据库并测试是否可用</p>
<p>初始化元数据库：</p>
<p><code>schematool -dbType mysql -initSchema</code></p>
<p>测试hive是否可用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive</span><br><span class="line">show datatables;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -u root -p;</span><br><span class="line">use hive</span><br><span class="line">show tables;</span><br></pre></td></tr></table></figure>

<p>出现如下界面则证明配置成功：</p>
<img src="/2019/11/03/hive%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/hive.jpg" style="zoom:75%;"></li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>配置</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop集群安装配置</title>
    <url>/2019/11/03/hadoop%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>配置步骤主要包括安装并配置jdk，设置SSH免密登录，安装并配置hadoop三步</p>
<p>详细步骤请阅读全文</p>
<a id="more"></a>

<p><em>以下步骤均需要在所有机子中进行操作</em></p>
<ol>
<li><p><strong>安装并配置jdk</strong></p>
<ol>
<li><p>下载jdk，并解压到usr/local目录下，解压命令:</p>
<p><code>tar -vxf jdk-8u151-linux-x64.tar.gz</code></p>
</li>
<li><p>配置java环境变量</p>
<p>打开/etc/profile文件</p>
<p><code>vi /etc/profile</code></p>
<p>在末尾添加java环境变量:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#set java enviroment</span><br><span class="line">export JAVA_HOME=/usr/local/java/jdk1.8.0_151 </span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/tools.jar </span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>使配置环境立即生效:</p>
<p><code>source /etc/profile</code></p>
</li>
<li><p>检查是否安装成功:</p>
<p><code>java -version</code></p>
<p>如果输出刚刚安装的jdk版本信息则安装成功，如果输出openjdk信息，说明安装的hadoop自带openjdk，需要将openjdk卸载，卸载后再次使环境变量生效，再次检查是否安装成功。</p>
</li>
</ol>
</li>
<li><p>设置SSH免密登录</p>
<ol>
<li><p>关闭防火墙:</p>
<p><code>systemctl stop firewalld</code></p>
<p><code>systemctl disable firewalld</code></p>
</li>
<li><p>关闭selinux</p>
<p>打开selinux文件:</p>
<p><code>vi /etc/sysconfig/selinux</code></p>
<p>将SELINUX的值改为disabled</p>
</li>
<li><p>修改hostname及host文件</p>
<p>修改hostname，一般一个机子做master，两个机子分别做slave1和slave2:</p>
<p><code>hostnamectl set-hostname xxx</code></p>
<p>修改/etc/hosts文件，将主机名和ip地址对应，命令如下：</p>
<p><code>vi /etc/hosts</code></p>
<p>在hosts中添加：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xxx.xxx.xx.xx master</span><br><span class="line">xxx.xxx.xx.xx slave1</span><br><span class="line">xxx.xxx.xx.xx slave2</span><br></pre></td></tr></table></figure>
</li>
<li><p>生成ssh密钥</p>
<p>输入如下命令生成key，不输入密码一直回车:</p>
<p><code>ssh-keygen -t rsa</code></p>
<p>/root文件夹下会自动生成.ssh文件夹</p>
</li>
<li><p>合并所有机器密钥</p>
<p>进入.ssh文件夹，将密钥写入到authorized_keys文件中，如果没有自动生成authorized_keys文件，自己创建即可，写入命令如下：</p>
<p><code>cat id_rsa.pub&gt;&gt;authorized_keys</code></p>
<p>将其他机子的密钥也合并到authorized_keys文件中</p>
</li>
<li><p>测试是否设置成功</p>
<p>输入:<code>ssh xxx</code></p>
<p>xxx为步骤2中设置的主机名，如果输入此条命令访问其他主机不需要输入密码，则证明设置成功。如果设置失败，在/var/logs文件夹下查看报错日志，常见的错误有root文件夹或.ssh文件夹权限错误，使用chmod命令修改权限即可。</p>
</li>
</ol>
</li>
<li><p>安装并配置hadoop</p>
<ol>
<li><p>下载hadoop，并解压到/hadoop文件夹下，解压命令:</p>
<p><code>tar -vxf hadoop-2.7.7.tar.gz</code></p>
</li>
<li><p>创建数据存放的文件夹:</p>
<p><code>mkdir /hadoop/tmp</code></p>
<p><code>mkdir /hadoop/hdfs</code></p>
<p><code>mkdir /hadoop/hdfs/data</code></p>
<p><code>mkdir /hadoop/hdfs/name</code></p>
</li>
<li><p>修改hadoop配置文件</p>
<p>修改hadoop-env.sh文件：在文件末尾添加:</p>
<p>export JAVA_HOME=/usr/local/java/jdk1.8.0_151（替换为你的jdk目录）</p>
<p>修改yarn-env.sh文件：在文件末尾添加:</p>
<p>export JAVA_HOME=/usr/local/java/jdk1.8.0_151（替换为你的jdk目录）</p>
<p>修改core-site.xml文件：在文件末尾添加:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;io.file.buffer.size&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;131072&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/hadoop/tmp&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>修改hdfs-site.xml文件：在文件末尾添加:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/hadoop/hdfs/name&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/hadoop/hdfs/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;slave1:9001&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>修改mapred-site.xml文件：在文件末尾添加:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>修改yarn-site.xml文件：在文件末尾添加:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8032&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8030&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8031&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8033&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;master:8088&lt;/value&gt;                                       </span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<hr>
</li>
<li><p>配置hadoop环境变量</p>
<p>打开/etc/profile文件:</p>
<p><code>vi /etc/profile</code></p>
<p>在末尾添加hadoop环境变量:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#set hadoop enviroment</span><br><span class="line">      export HADOOP_HOME=/hadoop/hadoop-2.7.7</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<p>使配置环境立即生效:</p>
<p><code>source /etc/profile</code></p>
</li>
</ol>
</li>
<li><p>启动hadoop</p>
<p>格式化namnode，命令如下：</p>
<p><code>hdfs namenode -format</code></p>
<p><code>start-all.sh</code></p>
<p>测试是否启动成功，在浏览器输入网址</p>
<p>master:50070（将master改为master节点机器的IP地址）</p>
<p>master:8080（将master改为master节点机器的IP地址）</p>
<p>所有机器均出现在master:50070网址中则证明启动成功，hdfs文件也可以在这里查看。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>配置</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
</search>
